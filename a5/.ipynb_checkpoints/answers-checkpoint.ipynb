{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)\n",
    "In character level embeddings we start with limited number of possible inputs (usually there is less characters that words in a language) so we don't need a lot of dimensions to represent them. For words, when embeddings are initialized randomly or pre-trained on some other data we need more dimensions to distinguish different meanings they can have. \n",
    "\n",
    "Later initial vectors representing characters are preprocessed in order to keep only the relevant information (for example max pooling is used).\n",
    "\n",
    "b) Total number of parameters:\n",
    "* character-based embedding model:\n",
    "\n",
    "    (V_char * e_char) + (f * e_char * k + f) + 2 * (e_char * e_char + e_char) \n",
    "\n",
    "* word-based lookup embedding model:\n",
    "\n",
    "    V_word * e_word\n",
    "    \n",
    "* comparison:\n",
    "\n",
    "For k = 5, V_word â‰ˆ 50, 000 and V_char = 96:\n",
    "\n",
    "\n",
    "Assuming that in the formula for the number of parameters in the character-based embedding model the greatest term is V_char * e_char we can say that the total number of parameters is no greater than 4 * V_char * e_char. \n",
    "So dividing V_word * e_word by (4 * V_char * e_char) gives us 130 * e_word / e_char. If e_word is around 5 times larger that e_char this will result in at least 650 times more parameters for word-embeddings model, so the magnitude is around 1 thousand. \n",
    "\n",
    "c) One advantage of using a convolutional architecture rather than a recurrent architecture for the purpose of generation of word embeddings:\n",
    "When convolution architecture is used we can use max pooling which is good at detecting specific patterns in the words, irrespective of surroundings characters (because the convolution is calculated only on a window of characters). \n",
    "\n",
    "d) \n",
    "* Max-pooling advantage: Max pooling takes into account only maximums, discarding most of the values from input. Because of this it's better at detecting specific pattern (e.g. for generation of word embeddings this could lead to detecting and using only input that is highly relevant for the meaning of the word).\n",
    "    \n",
    "* Average-pooling advantage: Average pooling takes into account only averages of input so it always uses all available values from input. This can be better in capturing overall characteristics of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_nmt_a5",
   "language": "python",
   "name": "local_nmt_a5"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
