{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model description and written questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)\n",
    "In character level embeddings we start with limited number of possible inputs (usually there is less characters that words in a language) so we don't need a lot of dimensions to represent them. For words, when embeddings are initialized randomly or pre-trained on some other data we need more dimensions to distinguish different meanings they can have. \n",
    "\n",
    "Later initial vectors representing characters are preprocessed in order to keep only the relevant information (for example max pooling is used).\n",
    "\n",
    "b) Total number of parameters:\n",
    "* character-based embedding model:\n",
    "\n",
    "    (V_char * e_char) + (f * e_char * k + f) + 2 * (e_char * e_char + e_char) \n",
    "\n",
    "* word-based lookup embedding model:\n",
    "\n",
    "    V_word * e_word\n",
    "    \n",
    "* comparison:\n",
    "\n",
    "For k = 5, V_word ≈ 50, 000 and V_char = 96:\n",
    "\n",
    "\n",
    "Assuming that in the formula for the number of parameters in the character-based embedding model the greatest term is V_char * e_char we can say that the total number of parameters is no greater than 4 * V_char * e_char. \n",
    "So dividing V_word * e_word by (4 * V_char * e_char) gives us 130 * e_word / e_char. If e_word is around 5 times larger that e_char this will result in at least 650 times more parameters for word-embeddings model, so the magnitude is around 1 thousand. \n",
    "\n",
    "c) One advantage of using a convolutional architecture rather than a recurrent architecture for the purpose of generation of word embeddings:\n",
    "When convolution architecture is used we can use max pooling which is good at detecting specific patterns in the words, irrespective of surroundings characters (because the convolution is calculated only on a window of characters). \n",
    "\n",
    "d) \n",
    "* Max-pooling advantage: Max pooling takes into account only maximums, discarding most of the values from input. Because of this it's better at detecting specific pattern (e.g. for generation of word embeddings this could lead to detecting and using only input that is highly relevant for the meaning of the word).\n",
    "    \n",
    "* Average-pooling advantage: Average pooling takes into account only averages of input so it always uses all available values from input. This can be better in capturing overall characteristics of words.\n",
    " \n",
    "## Implementation\n",
    "h) I have extended code of sanity_check.py - I check the output dimension. The code follows the same convention as the checks already provided in the assignment, it's also possible to run it multiple times without changes in Highway class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing NMT Systems "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \"traducir\": 5112,\n",
      "    \"traduce\": 8567,\n"
     ]
    }
   ],
   "source": [
    "! grep -E '\"traduzco\":|\"traduces\":|\"traduce\":|\"traduzca\":|\"traduzcas\":|\"traducir\":' vocab.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 2 forms in the vocabulary.\n",
    "### Why this is a bad thing for word-based NMT from Spanish to English. \n",
    "Because the missing words have similar meaning to the ones present in the vocabulary but we don't know it and will treat it as any other rare words and it will be difficult for the model to generate good translation of this word (even if there is some kind of pattern between Spanish and English versions like in the example above).\n",
    "\n",
    "\n",
    "### How our new character-aware NMT model may overcome this problem.\n",
    "We build embeddings on a character level. The expectation is then that the embeddings for the words that have similar meaning (e.g. traduces and traduce) will be similar and the model will pick up the their meaning and will be able to translate them correctly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \"traducen\": 23247,\n",
      "    \"traduce\": 8567,\n"
     ]
    }
   ],
   "source": [
    "! grep -E \"traduce\" vocab.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)\n",
    "#### Word2Vec:\n",
    "• financial - economic\n",
    "\n",
    "• neuron - nerve\n",
    "\n",
    "• Francisco - san\n",
    "\n",
    "• naturally - occuring\n",
    "\n",
    "• expectation - norms\n",
    "\n",
    "#### CharCNN:\n",
    "• financial - vertical\n",
    "\n",
    "• neuron - Newton\n",
    "\n",
    "• Francisco - France\n",
    "\n",
    "• naturally - practically\n",
    "\n",
    "• expectation - exception\n",
    "\n",
    "\n",
    "#### What kind of similarity is modeled by Word2Vec. \n",
    "It's semantic similarity (based on similar meaning of words).\n",
    "\n",
    "#### What kind of similarity is modeled by the CharCNN.\n",
    "\n",
    "It's very often similary based on characters but also semantic similarity.\n",
    "\n",
    "#### How the differences in the methodology of Word2Vec and a CharCNN explain the differences you have found.\n",
    "\n",
    "Word2Vec treats all words in the same way irrespective of the fact if they look similar or not and embeddings are built only using information about co-ocurrence in the input documents. CharCNN calculates convolution on characters embeddings so for similar words (in terms of the characters they contain) we can expect the output to be similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c)\n",
    "#### Example where the character-based decoder produced an acceptable translation in place of UNK\n",
    "    \n",
    "#### Example where the character-based decoder produced an incorrect translation in place of UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I'm <unk> that we have never come to know us.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_nmt_a5",
   "language": "python",
   "name": "local_nmt_a5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
